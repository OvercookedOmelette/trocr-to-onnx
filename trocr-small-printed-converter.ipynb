{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info\n",
    "\n",
    "This notebook allows us to export some TrOCR models to the ONNX format.\n",
    "\n",
    "NOTE: A lot of the code is from a notebook somebody else created for converting MBart models to ONNX, which is itself following fastT5, a library to convert T5 models to ONNX.\n",
    "See: https://github.com/Ki6an/fastT5/issues/7\n",
    "\n",
    "NOTE: Doubt this works for training (removed some logic relating to loss calculations). This should probably only be used for inference.\n",
    "Additional NOTE: Model seems to perform poorly when optimized (not to be confused with quantized). Not sure why?\n",
    "\n",
    "NOTE: Quantization seems to have a pretty bad effect on the model quality. Not sure if it's worth it. Only tested on single images.\n",
    "\n",
    "Probably only understand ~50% of this code. Welcome to hell.\n",
    "\n",
    "Some possible things to fix in the future:\n",
    "- Giving proper names to some of the dynamic axes in `generate_onnx_representation`. Had to give some of them differing names, otherwise we get shape mismatching errors when trying to run inference.\n",
    "- Adding the ability to return the loss in the ONNX models, for use in training.\n",
    "\n",
    "Tested on `microsoft/trocr-small-printed` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "import functools\n",
    "import operator\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_export = True\n",
    "run_quantize = False\n",
    "run_inference = True\n",
    "\n",
    "huggingface_model = 'microsoft/trocr-small-printed' # The Huggingface model name/path to load and export.\n",
    "\n",
    "# The path to export all of the files to.\n",
    "output_folder = './'\n",
    "\n",
    "# The path of a sample image to ensure that the ONNX model is giving the proper output.\n",
    "image_path = './sample.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exported_paths():\n",
    "    return f'{output_folder}/encoder.onnx', f'{output_folder}/decoder.onnx', f'{output_folder}/decoder_init.onnx'\n",
    "\n",
    "def get_quantized_paths():\n",
    "    return f'{output_folder}/encoder_q.onnx', f'{output_folder}/decoder_q.onnx', f'{output_folder}/decoder_init_q.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(torch.nn.Module):\n",
    "    \"\"\" Creation of a class to output only the last hidden state from the encoder \"\"\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, *input, **kwargs):\n",
    "        # self.encoder() returns a BaseModelOutput object\n",
    "        # output[0] = last hidden state for encoder (this is what we care about)\n",
    "        # output[1] = pooler state (not what we want for TrOCR)\n",
    "        output = self.encoder(*input, **kwargs)\n",
    "        return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionDecoderWithLMhead(torch.nn.Module):\n",
    "    def __init__(self, decoder, config):\n",
    "        # Unlike the original notebook from the Github link, we don't (probably?) need to add the LM logit bias for TrOCR.\n",
    "        # The LM head for TrOCR is already baked in the decoder thankfully.\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        input_ids, attention_mask, encoder_hidden_states = inputs[:3]\n",
    "\n",
    "        # convert flattened past_key_values inputs to tuples that the\n",
    "        # transformers decoder expects\n",
    "        list_pkv = inputs[3:]\n",
    "        past_key_values = tuple(list_pkv[i : i + 4] for i in range(0, len(list_pkv), 4))\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True, # Some TrOCR models don't need this enabled (maybe it's already enabled for those models?), whereas others do.\n",
    "        )\n",
    "\n",
    "        return decoder_output[0], decoder_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionDecoderWithLMheadInitial(torch.nn.Module):\n",
    "    def __init__(self, decoder, config):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, encoder_hidden_states):\n",
    "        decoder_output = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "        return decoder_output[0], decoder_output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_model_into_encoder_decoder(model):\n",
    "    encoder = model.get_encoder()\n",
    "    decoder = model.get_decoder()\n",
    "\n",
    "    # No need for the output embeddings - it's already baked in the decoder.\n",
    "\n",
    "    simplified_encoder = VisionEncoder(encoder)\n",
    "    decoder_with_lm_head = VisionDecoderWithLMhead(decoder, model.config)\n",
    "    decoder_with_lm_head_init = VisionDecoderWithLMheadInitial(decoder, model.config)\n",
    "\n",
    "    return simplified_encoder, decoder_with_lm_head, decoder_with_lm_head_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_onnx_representation(model, tokenizer, feature_extractor):\n",
    "    (\n",
    "        simplified_encoder,\n",
    "        decoder_with_lm_head,\n",
    "        decoder_with_lm_head_init,\n",
    "    ) = turn_model_into_encoder_decoder(model)\n",
    "\n",
    "    model_config = model.config\n",
    "\n",
    "    encoder_path, decoder_path, init_decoder_path = get_exported_paths()\n",
    "\n",
    "    # creating dummy inputs\n",
    "    sample_encoder_input = torch.randn((3, 224, 224)) # C * H * W\n",
    "    pixel_values = feature_extractor(sample_encoder_input, return_tensors='pt').pixel_values\n",
    "\n",
    "    sample_decoder_input = \"The universe is a dark forest.\"\n",
    "    model_decoder_inputs = tokenizer(sample_decoder_input, return_tensors=\"pt\")\n",
    "    input_ids = model_decoder_inputs[\"input_ids\"]\n",
    "\n",
    "    batch_size = 5\n",
    "\n",
    "    # TrOCR stores most of the important properties in the encoder and decoder configs.\n",
    "    decoder_attention_heads = model_config.decoder.num_attention_heads\n",
    "    d_model = model_config.decoder.hidden_size\n",
    "    d_enc_model = model_config.encoder.hidden_size\n",
    "    num_decoder_layers = model_config.decoder.num_hidden_layers\n",
    "\n",
    "    n_heads = decoder_attention_heads\n",
    "    seq_length_a, seq_length_b = input_ids.shape\n",
    "\n",
    "    d_kv = d_model // decoder_attention_heads\n",
    "\n",
    "    input_ids_dec = torch.ones((batch_size, 1), dtype=torch.int64)\n",
    "    attention_mask_dec = torch.ones((batch_size, seq_length_b), dtype=torch.int64)\n",
    "    enc_out = torch.ones(\n",
    "        (batch_size, seq_length_b, d_enc_model), dtype=torch.float32\n",
    "    )\n",
    "    sa = torch.ones(\n",
    "        (batch_size, n_heads, seq_length_a, d_kv), dtype=torch.float32\n",
    "    )\n",
    "    ca = torch.ones(\n",
    "        (batch_size, n_heads, seq_length_b, d_kv), dtype=torch.float32\n",
    "    )\n",
    "    # (self attention keys, self attention values, cross attention keys, cross attention values)\n",
    "    attention_block = (sa, sa, ca, ca)\n",
    "    past_key_values = (attention_block,) * num_decoder_layers\n",
    "    flat_past_key_values = functools.reduce(operator.iconcat, past_key_values, [])\n",
    "\n",
    "    decoder_all_inputs = tuple(\n",
    "        [input_ids_dec, attention_mask_dec, enc_out] + flat_past_key_values\n",
    "    )\n",
    "    num_of_inputs = 4 * num_decoder_layers\n",
    "\n",
    "    # Exports to ONNX\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # export decoder\n",
    "        decoder_inputs = [\n",
    "            \"input_ids\",\n",
    "            \"encoder_attention_mask\",\n",
    "            \"encoder_hidden_states\",\n",
    "        ]\n",
    "        pkv_input_names = [\n",
    "            f\"l{i//4}_{'self' if i % 4 < 2 else 'cross'}_{'keys' if not i % 2 else 'vals'}\"\n",
    "            for i in range(num_of_inputs)\n",
    "        ]\n",
    "        decoder_input_names = decoder_inputs + pkv_input_names\n",
    "\n",
    "        decoder_outputs = [\"logits\"]\n",
    "        pkv_output_names = [\n",
    "            f\"l{i//4}_{'self_out' if i % 4 < 2 else 'cross'}_{'keys' if not i % 2 else 'vals'}\"\n",
    "            for i in range(num_of_inputs)\n",
    "        ]    \n",
    "        decoder_output_names = decoder_outputs + pkv_output_names\n",
    "\n",
    "        dyn_axis = {\n",
    "            \"input_ids\": {0: \"batch\", 1: \"input_ids_dim1\"},\n",
    "            \"encoder_attention_mask\": {0: \"batch\", 1: \"encoder_attention_mask_dim1\"},\n",
    "            \"encoder_hidden_states\": {0: \"batch\", 1: \"seq_length\"},\n",
    "            \"logits\": {0: \"batch\", 1: \"seq_length\"},\n",
    "        }\n",
    "        dyn_pkv = {\n",
    "            name: {0: \"batch\", 2: \"seq_length\"} for name in pkv_input_names + pkv_output_names\n",
    "        }\n",
    "        dyn_axis_params = {**dyn_axis, **dyn_pkv}\n",
    "\n",
    "        # encoder\n",
    "        torch.onnx.export(\n",
    "            simplified_encoder,\n",
    "            args=(pixel_values),\n",
    "            f=Path(encoder_path).as_posix(),\n",
    "            export_params=True,\n",
    "            opset_version=12,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[\"pixel_values\"],\n",
    "            output_names=[\"hidden_states\"],\n",
    "            dynamic_axes={\n",
    "                \"pixel_values\": {0: \"batch\", 1: \"pixel_values_dim1\"},\n",
    "                \"hidden_states\": {0: \"batch\", 1: \"seq_length\"},\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # decoder\n",
    "        torch.onnx.export(\n",
    "            decoder_with_lm_head,\n",
    "            decoder_all_inputs,\n",
    "            Path(decoder_path).as_posix(),\n",
    "            export_params=True,\n",
    "            do_constant_folding=False,\n",
    "            opset_version=12,\n",
    "            input_names=decoder_input_names,\n",
    "            output_names=decoder_output_names,\n",
    "            dynamic_axes=dyn_axis_params,\n",
    "        )\n",
    "        \n",
    "        # initial decoder to produce past key values\n",
    "        torch.onnx.export(\n",
    "            decoder_with_lm_head_init,\n",
    "            (input_ids_dec, attention_mask_dec, enc_out),\n",
    "            Path(init_decoder_path).as_posix(),\n",
    "            export_params=True,\n",
    "            do_constant_folding=False,\n",
    "            opset_version=12,\n",
    "            input_names=[\n",
    "                \"input_ids\",\n",
    "                \"encoder_attention_mask\",\n",
    "                \"encoder_hidden_states\",\n",
    "            ],\n",
    "            output_names=decoder_output_names,\n",
    "            dynamic_axes={\n",
    "                \"logits\": {0: \"batch\", 1: \"seq_length\"},\n",
    "                \"input_ids\": {0: \"batch\", 1: \"input_ids_dim1\"},\n",
    "                \"encoder_attention_mask\": {0: \"batch\", 1: \"encoder_attention_mask_dim1\"},\n",
    "                \"encoder_hidden_states\": {0: \"batch\", 1: \"seq_length\"},\n",
    "                **{\n",
    "                    name: {0: \"batch\", 2: \"seq_length\"} for name in pkv_output_names\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return encoder_path, decoder_path, init_decoder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_export:\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(huggingface_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(huggingface_model)\n",
    "\n",
    "    onnx_model_paths = generate_onnx_representation(model, tokenizer, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "def quantize(onnx_model_path, output_path):\n",
    "    quantize_dynamic(\n",
    "        model_input=onnx_model_path,\n",
    "        model_output=output_path,\n",
    "        per_channel=True,\n",
    "        weight_type=QuantType.QUInt8,\n",
    "        reduce_range=True,\n",
    "    )\n",
    "\n",
    "    print('Quantized a model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_quantize:\n",
    "    enc, dec, dec_init = get_exported_paths()\n",
    "    enc_out, dec_out, dec_init_out = get_quantized_paths()\n",
    "\n",
    "    quantize(enc, enc_out)\n",
    "    quantize(dec, dec_out)\n",
    "    quantize(dec_init, dec_init_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.modeling_outputs import (\n",
    "    Seq2SeqLMOutput,\n",
    "    BaseModelOutput,\n",
    ")\n",
    "\n",
    "class OnnxVisionEncoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_sess):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_sess\n",
    "\n",
    "        self.main_input_name = 'pixel_values'\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        encoder_hidden_state = torch.from_numpy(\n",
    "            self.encoder.run(\n",
    "                None,\n",
    "                {\n",
    "                    \"pixel_values\": pixel_values.cpu().numpy(),\n",
    "                },\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        return BaseModelOutput(encoder_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxVisionDecoder(torch.nn.Module):\n",
    "    def __init__(self, decoder_sess):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder_sess\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, encoder_hidden_states, past_key_values, output_attentions=None, output_hidden_states=None, return_dict=None, encoder_attention_mask=None, **kwargs):\n",
    "\n",
    "        decoder_inputs = {\n",
    "            'input_ids': input_ids.detach().cpu().numpy(),\n",
    "            'encoder_attention_mask': encoder_attention_mask.detach().cpu().numpy(),\n",
    "            \"encoder_hidden_states\": encoder_hidden_states.cpu().numpy(),\n",
    "        }\n",
    "        flat_past_key_values = functools.reduce(operator.iconcat, past_key_values, [])\n",
    "        \n",
    "        input_names = [x.name for x in self.decoder.get_inputs()]\n",
    "        inputs = [\n",
    "            input_ids.detach().cpu().numpy(),\n",
    "            encoder_attention_mask.cpu().numpy(),\n",
    "        ] + [\n",
    "            tensor.cpu().numpy() for tensor in flat_past_key_values\n",
    "        ]\n",
    "\n",
    "        decoder_inputs = dict(zip(input_names, inputs))\n",
    "        decoder_outputs = self.decoder.run(None, decoder_inputs)\n",
    " \n",
    "        list_pkv = tuple(torch.from_numpy(x) for x in decoder_outputs[1:])\n",
    "        out_past_key_values = tuple(\n",
    "            list_pkv[i : i + 4] for i in range(0, len(list_pkv), 4)\n",
    "        )\n",
    "\n",
    "        return torch.from_numpy(decoder_outputs[0]), out_past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxVisionDecoderInit(torch.nn.Module):\n",
    "    def __init__(self, decoder_sess):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder_sess\n",
    "\n",
    "    def forward(self, input_ids, encoder_hidden_states, attention_mask, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs):\n",
    "        decoder_outputs = self.decoder.run(\n",
    "            None,\n",
    "            {\n",
    "                'input_ids': input_ids.detach().cpu().numpy(),\n",
    "                'encoder_attention_mask': attention_mask.detach().cpu().numpy(),\n",
    "                \"encoder_hidden_states\": encoder_hidden_states.cpu().numpy(),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        list_pkv = tuple(torch.from_numpy(x) for x in decoder_outputs[1:])\n",
    "        out_past_key_values = tuple(\n",
    "            list_pkv[i : i + 4] for i in range(0, len(list_pkv), 4)\n",
    "        )\n",
    "\n",
    "        return torch.from_numpy(decoder_outputs[0]), out_past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_utils import GenerationMixin\n",
    "\n",
    "class OnnxVision(GenerationMixin):\n",
    "    def __init__(self, config, encoder_sess, decoder_sess, decoder_sess_init):\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = OnnxVisionEncoder(encoder_sess)\n",
    "        self.decoder = OnnxVisionDecoder(decoder_sess)\n",
    "        self.decoder_init = OnnxVisionDecoderInit(decoder_sess_init)\n",
    "\n",
    "        self.main_input_name = 'pixel_values'\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n",
    "    ):\n",
    "\n",
    "        def _prepare_inputs_for_generation(input_ids, past=None, attention_mask=None, **model_kwargs):\n",
    "            input_shape = input_ids.shape\n",
    "            # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "            if attention_mask is None:\n",
    "                attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "            # cut decoder_input_ids if past is used\n",
    "            if past is not None:\n",
    "                input_ids = input_ids[:, -1:]\n",
    "\n",
    "            return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
    "\n",
    "        decoder_inputs = _prepare_inputs_for_generation(input_ids, past=past)\n",
    "        decoder_attention_mask = decoder_inputs[\"attention_mask\"] if \"attention_mask\" in decoder_inputs else None\n",
    "        input_dict = {\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"decoder_input_ids\": decoder_inputs[\"input_ids\"],\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": decoder_inputs[\"past_key_values\"],\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "        return input_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return \"cpu\"\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"You have to specify pixel_values\")\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            # (when using generate, we already get encoder_outputs generated\n",
    "            #  by _prepare_encoder_decoder_kwargs_for_generation)\n",
    "            encoder_outputs = self.encoder(\n",
    "                pixel_values,\n",
    "            )\n",
    "\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        # optionally project encoder_hidden_states\n",
    "        if (\n",
    "            self.config.encoder.hidden_size != self.config.decoder.hidden_size\n",
    "            and self.config.decoder.cross_attention_hidden_size is None\n",
    "        ):\n",
    "            encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n",
    "\n",
    "        if past_key_values is None:\n",
    "            init_decoder_outputs = self.decoder_init(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=decoder_attention_mask,\n",
    "            )\n",
    "\n",
    "            logits, past_key_values = init_decoder_outputs\n",
    "        else:\n",
    "            encoder_attention_mask = torch.ones((encoder_hidden_states.shape[0], encoder_hidden_states.shape[1]), dtype=torch.int64)\n",
    "\n",
    "            logits, past_key_values = self.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                attention_mask=decoder_attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "\n",
    "        outputs = Seq2SeqLMOutput(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import (\n",
    "    GraphOptimizationLevel,\n",
    "    InferenceSession,\n",
    "    SessionOptions,\n",
    ")\n",
    "\n",
    "def get_onnx_runtime_sessions(\n",
    "    path_to_encoder,\n",
    "    path_to_decoder,\n",
    "    path_to_initial_decoder,\n",
    "    provider=[\n",
    "        \"CPUExecutionProvider\",\n",
    "    ],\n",
    ") -> InferenceSession:\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    options.log_severity_level = 0\n",
    "\n",
    "    encoder_sess = InferenceSession(path_to_encoder, options, provider)\n",
    "    encoder_sess.disable_fallback()\n",
    "\n",
    "    decoder_sess = InferenceSession(path_to_decoder, options, provider)\n",
    "    decoder_sess.disable_fallback()\n",
    "\n",
    "    decoder_sess_init = InferenceSession(path_to_initial_decoder, options, provider)\n",
    "    decoder_sess_init.disable_fallback()\n",
    "\n",
    "    return encoder_sess, decoder_sess, decoder_sess_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "def try_inference(model, image, tokenizer, feature_extractor):\n",
    "    f = feature_extractor(image, return_tensors='pt').pixel_values\n",
    "\n",
    "    tokens = model.generate(\n",
    "        f,\n",
    "    )\n",
    "\n",
    "    output = tokenizer.batch_decode(tokens)[0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_inference:\n",
    "    # Load utils.\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(huggingface_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    config = AutoConfig.from_pretrained(huggingface_model)\n",
    "    hf_model = VisionEncoderDecoderModel.from_pretrained(huggingface_model)\n",
    "\n",
    "    # Load ONNX model.\n",
    "    encoder_path, decoder_path, decoder_init_path = get_quantized_paths() if run_quantize else get_exported_paths()\n",
    "    encoder_sess, decoder_sess, decoder_init_sess = get_onnx_runtime_sessions(encoder_path, decoder_path, decoder_init_path)\n",
    "    onnx_model = OnnxVision(config, encoder_sess, decoder_sess, decoder_init_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if run_inference:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    onnx_output_text = try_inference(onnx_model, image, tokenizer, feature_extractor)\n",
    "    print('ONNX:')\n",
    "    print(onnx_output_text)\n",
    "\n",
    "    print('PYTORCH:')\n",
    "    pytorch_output_text = try_inference(hf_model, image, tokenizer, feature_extractor)\n",
    "    print(pytorch_output_text)\n",
    "\n",
    "    print(pytorch_output_text == onnx_output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7089c5a6489b66f0c45dd1b11cd4de66791717d1377346002259d9d32d6aa691"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
